{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepre = 'data/1'\n",
    "imnames = os.listdir(filepre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(image, windowname = 'image'):\n",
    "    cv2.imshow(windowname, image)\n",
    "    keypress = cv2.waitKey(0)\n",
    "    if keypress:\n",
    "        cv2.destroyWindow(windowname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *target_shape):\n",
    "        super().__init__()\n",
    "        self.target_shape = target_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.view(*self.target_shape)\n",
    "\n",
    "class HandGestureClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pipeline = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size = 5), #1*96*128 -> 4*92*124\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels=4, out_channels=16, kernel_size = 5), #4*92*124 -> 16*88*120\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size = 7), #16*88*120 -> 32*82*114\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=7), #32*82*114 -> 32*76*108\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=4, kernel_size=7), #32*76*108 -> 4*70*102\n",
    "            Reshape(-1, 4*70*102),\n",
    "            nn.Linear(in_features=4*70*102, out_features=128),\n",
    "            nn.Dropout(p=.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.Dropout(p=.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=6),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.pipeline(X)\n",
    "\n",
    "net = HandGestureClassifier()\n",
    "net.load_state_dict(torch.load('models/classifier_5.pt', map_location=torch.device('cpu') ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft(image):\n",
    "    return np.fft.fft(image).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from scipy.signal import correlate2d\n",
    "\n",
    "def canny_norm_filter(image):\n",
    "    im = norm(image, axis = -1)/((3*(255**2))**.5)\n",
    "    return cv2.Canny( (im*255).astype(np.uint8), 100, 150)\n",
    "\n",
    "def canny_color_filter(image):\n",
    "    return cv2.Canny(image, 100, 150)\n",
    "\n",
    "def downsample_filter(image, by = 5):\n",
    "    return image[::by,::by,:]\n",
    "\n",
    "def difference_kernel(image, size = 3):\n",
    "    im = np.moveaxis(image, -1, 0)\n",
    "\n",
    "    size = 3\n",
    "    gradient_kernel = -np.ones([size, size])\n",
    "    gradient_kernel /= size**2\n",
    "    gradient_kernel[ int(size/2) ][ int(size/2) ] *= -1\n",
    "    \n",
    "    im = np.stack(correlate2d(im[a], gradient_kernel, boundary = 'symm') for a in range(3) )\n",
    "\n",
    "    return np.moveaxis(im, 0, -1).astype(np.uint8)\n",
    "\n",
    "def mean_denoiser(image, mark = False):\n",
    "    '''\n",
    "        Finds mean x, y as well as avg distance from mean along x and y axes.\n",
    "        zeros out everything beyond a certain distance from mean as determined by\n",
    "        x and y marginal distance\n",
    "    '''\n",
    "    py = np.arange(image.shape[0] )\n",
    "    vy = image.sum(axis = 1)\n",
    "    px = np.arange(image.shape[1] )\n",
    "    vx = image.sum(axis = 0)\n",
    "        \n",
    "    ymean = int( (py @ vy)/vy.sum() if vy.sum() != 0 else 0 )\n",
    "    xmean = int( (px @ vx)/vx.sum() if vx.sum() != 0 else 0 )\n",
    "        \n",
    "    marginal_x = image.sum(axis = 0)/max(image.sum(), 1)\n",
    "    marginal_y = image.sum(axis = 1)/max(image.sum(), 1)\n",
    "    \n",
    "    dy = abs(np.arange(image.shape[0]) - ymean )\n",
    "    dx = abs(np.arange(image.shape[1]) - xmean )\n",
    "    \n",
    "    sy = np.mean( dy * marginal_y )\n",
    "    sx = np.mean( dx * marginal_x )\n",
    "    \n",
    "    py = image.shape[0] * sy * 5\n",
    "    px = image.shape[1] * sx * 5\n",
    "    \n",
    "    image = np.moveaxis(np.stack([image]*3), 0, -1)\n",
    "    \n",
    "    if mark:\n",
    "        image[ymean, xmean] = [255, 0, 0]\n",
    "        cv2.rectangle(\n",
    "            image[:,:,0], \n",
    "            ( int(xmean - px/2), int(ymean - py/2) ), \n",
    "            ( int(xmean + px/2), int(ymean + py/2) ), \n",
    "            (255,0,0), \n",
    "            2)\n",
    "    \n",
    "    mask = np.ones(image.shape[:2])\n",
    "    mask[ \n",
    "        max(0, int(ymean - py/2) ) : min(int(ymean + py/2), image.shape[0] ), \n",
    "        max(0, int(xmean - px/2) ) : min(int(xmean + px/2), image.shape[1] )\n",
    "    ] = 0\n",
    "    \n",
    "    image[mask.astype(bool) ] = 0\n",
    "        \n",
    "    return image\n",
    "\n",
    "def mean_center(image):\n",
    "    image = image[:,:,0]\n",
    "    \n",
    "    py = np.arange(image.shape[0] )\n",
    "    vy = image.sum(axis = 1)\n",
    "    px = np.arange(image.shape[1] )\n",
    "    vx = image.sum(axis = 0)\n",
    "        \n",
    "    ymean = int( (py @ vy)/vy.sum() if vy.sum() != 0 else 0 )\n",
    "    xmean = int( (px @ vx)/vx.sum() if vx.sum() != 0 else 0 )\n",
    "    \n",
    "    y, x = np.where(image)\n",
    "    y_centered = y - ymean + int(image.shape[0]/2)\n",
    "    x_centered = x - xmean + int(image.shape[1]/2)\n",
    "\n",
    "    y_centered = np.clip(y_centered, 0, image.shape[0]-1)\n",
    "    x_centered = np.clip(x_centered, 0, image.shape[1]-1)\n",
    "\n",
    "    image = np.zeros(image.shape[:2])\n",
    "    image[y_centered, x_centered] = 1\n",
    "\n",
    "    image = np.moveaxis(np.stack([image]*3), 0, -1).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "def contour_image(image):\n",
    "    image = image.max(axis = -1)\n",
    "    # Run findContours - Note the RETR_EXTERNAL flag\n",
    "    # Also, we want to find the best contour possible with CHAIN_APPROX_NONE\n",
    "    contours, hierarchy = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    # Create an output of all zeroes that has the same shape as the input\n",
    "    # image\n",
    "    out = np.zeros_like(image)\n",
    "\n",
    "    # On this output, draw all of the contours that we have detected\n",
    "    # in white, and set the thickness to be 3 pixels\n",
    "    cv2.drawContours(out, contours, -1, 255, 1)\n",
    "    \n",
    "    # Spawn new windows that shows us the donut\n",
    "    # (in grayscale) and the detected contour\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torchmodel_annotate(image):\n",
    "    with torch.no_grad():\n",
    "        if len(image.shape) == 3:\n",
    "            net_image = image[:,:,0]\n",
    "\n",
    "        net.eval()\n",
    "        net_image = image.reshape([1,1,96,128])\n",
    "        net_image = torch.tensor(net_image).float()\n",
    "        res = net(net_image)\n",
    "    predicted = np.argmax(res[0])\n",
    "    conf = res[0][predicted]\n",
    "    cv2.putText(\n",
    "        image, \"Gesture: {}, Confidence {}\".format(predicted, conf.exp() ), (0,13), \n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=.25,\n",
    "        color = (255,0,0), thickness=1\n",
    "    )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torchmodel_coefs(image):\n",
    "    with torch.no_grad():\n",
    "        if len(image.shape) == 3:\n",
    "            net_image = image[:,:,0]\n",
    "\n",
    "        net.eval()\n",
    "        net_image = image.reshape([1,1,96,128])\n",
    "        net_image = torch.tensor(net_image).float()\n",
    "        res = net(net_image)\n",
    "    predicted = np.argmax(res[0])\n",
    "    conf = res[0][predicted]\n",
    "\n",
    "    return predicted, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_filter(*image_pipeline):\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    cv2.namedWindow('Main', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Main', (640, 480) )\n",
    "    cv2.namedWindow('Old', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Old', (640, 480) )\n",
    "        \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            cv2.imshow('Old', frame)\n",
    "            for filter in image_pipeline:\n",
    "                frame = filter(frame)\n",
    "\n",
    "            cv2.imshow('Main', frame)\n",
    "        else:\n",
    "            print('No frame found')\n",
    "                \n",
    "        keypress = cv2.waitKey(1)\n",
    "        if keypress == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_annotate():\n",
    "    filters = [downsample_filter, canny_norm_filter, mean_denoiser, mean_center, contour_image]\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    \n",
    "    cv2.namedWindow('Main', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Main', (640, 480) )\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        process_frame = frame.copy()\n",
    "        for filter in filters:\n",
    "            process_frame = filter(process_frame)\n",
    "        \n",
    "        gesture, coef = torchmodel_coefs(process_frame)\n",
    "        \n",
    "        cv2.putText(\n",
    "            frame, \"Gesture: {}, Confidence {}\".format(gesture, coef.exp() ), (0,25), \n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1,\n",
    "            color = (255,0,0), thickness=2\n",
    "        )\n",
    "        \n",
    "        if ret:\n",
    "            cv2.imshow('Main', frame)\n",
    "        else:\n",
    "            print('No Frame Found')\n",
    "        \n",
    "        keypress = cv2.waitKey(1)\n",
    "        if keypress == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_annotate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (96, 128) dimension\n",
    "video_filter(downsample_filter, canny_norm_filter, mean_denoiser, mean_center, contour_image, torchmodel_annotate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
